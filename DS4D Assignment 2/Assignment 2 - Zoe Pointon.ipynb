{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Entertainment Data (CleverClogs)\n",
    "\n",
    "\n",
    "Author: Zoe Pointon\n",
    "\n",
    "Context: User Experience\n",
    "\n",
    "Background information: https://www.blackwoodgroup.org.uk/clevercogs/\n",
    "\n",
    "The data analysed in this notebook is the usage data for a tablet application called 'CleverClogs'. CleverClogs is an application, developed for a tablet, designed to empower elderly and disabled people with technology, and incorporate it into their daily lives. The user competency of the app is varied, however all are given 12 hours of training. The app is multi functional. There are sections within the app for; music, video, games, video calling, panic alarm, calendar and more. The app also has a web browser (some extremely vulnerable users have this switched off).\n",
    "\n",
    "\n",
    "The dataset is made up of click data. Each row in the dataset has these seven columns; day/time stamp, user ID, user role, building, link title, link type and content info. There are around 500 users and 132537 lines of data.\n",
    "\n",
    "We were later also given a dataset of the users information. This data included details suchas; external ID, clever clogs user ID, birthdate, gender and condition. There are in total 696 users. The two datasets are linked via an ID specifically the ExternalID.\n",
    "\n",
    "The data owner is the man who developed the tablet, Collin, and a university researcher, Lynda. They would like the data to be analysed so they can include some visulisations for a proposal they are writing. They are interested in any insight that we can gain from the dataset.\n",
    "\n",
    "My main focus is to find out how the users are using the app. Specifically, finding trends in what the users are using the app for. Also finding out how they are accessing these things. When are they using the built in functions? And when do they use the web browser?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-13f6a3793411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Click Data Cleaning\n",
    "\n",
    "I first ran a .shape to make sure it returned the right number of columns and rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This showed me that the data was returning more columns than it should. There are only 7 columns in this data. To make sure i printed all the column headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Column headers', df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This told me that there were some extra unamed rows I needed to remove. So I droped all columns except the first 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.columns[7:], axis=1, inplace=True)\n",
    "print('Column headers', df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then ran value counts on some of the rows with more quantative data to make sure all rows were returning valid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Building'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LinkType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LinkTitle'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ContentInfo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could see that there were clearly 18 rows that were returning invalid data. I removed these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df['Role']==' font-family: \" comic=\"\" sans=\"\" ms\"'].index, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then ran value count for the Role column again to make sure it only returned valid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Role'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ContentInfo was returning some very messy values so I went through and cleaned them up using replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|329|10| Comfort Break|1|': 'Comfort Break'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|424|10| Contact Blackwood|1|': 'Contact Blackwood'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|332|5| Dropped Book|1|': 'Dropped Book'})    \n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'<h2>Care Standards for support services </h2><div><span style=\"font-weight: normal': 'Care Standards for support services'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|479|10| Ask for new content|1|': 'Ask for new content'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|209|30| Alarm|1|': 'Alarm'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|356|15| Out of Bed (2)|1|1': 'Out of Bed'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|224|10| Coffee|1|': 'Coffee'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|348|15| HELP INTO BED|1|1': 'HELP INTO BED'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|454|2|Slow to Drain|1|': 'Slow to Drain'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|331|10|  Breakfast  |1|': 'Breakfast'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|327|5| Cup of Tea |1|': 'Cup of Tea'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|295|10| Wheel Chair Support|1|': 'Wheel Chair Support'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|542|30|Blocked|1|': 'Blocked'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'-1|331|10|  Breakfast  |1|': 'Breakfast'})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'Make a Payment<div><br></div><div><br></div>': 'Make a Payment'})\n",
    "\n",
    "        \n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'=\"\" font-size:=\"\" large': np.nan})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'66|331||||': np.nan})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'<iframe src=\"spiral.asp\" height=\"600\" width=\"700\" scrolling=\"no\" frameBorder=\"0\"></iframe>': np.nan})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'If the water is not draining properly try these steps first<div><br></div><div>1</div><div><br></div><div><br></div><div>2</div><div><br></div><div><br></div><div>3</div>': np.nan})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'<p class=\"MsoNormal\" style=\"margin: 0px': np.nan})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'<span style=\"font-family: &quot': np.nan})\n",
    "df['ContentInfo'] = df['ContentInfo'].replace({'<div style=\"text-align: center': np.nan})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ContentInfo'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then decided to remove all rows that are about support accounts. I did this as I am not interested in how support people are using the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df['Role']=='Support'].index, inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then a value count to make sure it only returned User roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LinkType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Data Cleaning\n",
    "Now I am going to clean the user-data.csv dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud = pd.read_csv(\"user-data.csv\")\n",
    "print(ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Condition'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While trying to work out age of users I found that there was a user without a BirthDate inputed. I dropped this row becuase it was disturbing my dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud.drop(ud.loc[ud['BirthDate'] == ''].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to look ok from the value counts I ran. So I am now going to work out each users age and create a new column form this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['BirthDate'] =  pd.to_datetime(ud['BirthDate'], format='%d/%m/%Y')\n",
    "print(ud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ud.index:\n",
    "    today = datetime.datetime.now()\n",
    "    dob = ud.at[i, 'BirthDate']\n",
    "    ud.at[i, 'Age'] = today.year - dob.year - ((today.month, today.day) < (dob.month, dob.day))\n",
    "print(ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Click Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I explored the first dataset called df. As my angle of analitics was the LinkType column I satrted there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I refreshed my memory on the diffrent vaules of LinkType."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LinkType'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a bar chart to see what types of things the users are clicking on. \n",
    "\n",
    "The results showed me that users are using the internet about the same amount as they are using the built in tablet fuctions (Category). This could indicate that there is some built functions missing or that do not perform as the user would wish so they are using the internet instead. It could also indicated that users just like browsing the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LinkType'].value_counts().plot(kind='barh')\n",
    "print('Figure 1: Sections')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of what I want to find out is how the clients are using the the custom built sections and for what. Because of this I created a barchat to see what Category sections are the most popular. I looked at the LinkTitles again. These are link titles of all aspects of the tabet. So I needed to narrow down the return to just the 'Category' LinkTitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LinkTitle'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I cut the dataset to return only rows where LinkType equals 'Categorys'. I then added all the LinkTitles to a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CategoryData = []\n",
    "for i in df.index:\n",
    "    if df.at[i, 'LinkType'] == 'Category':\n",
    "        CategoryData.append(df.at[i, 'LinkTitle'])\n",
    "\n",
    "print(CategoryData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I counted how many items there were in the list to get an idea of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CategoryData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then keyed and counted the values in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "CategoryDataCounter=collections.Counter(CategoryData)\n",
    "\n",
    "print(CategoryDataCounter.keys())\n",
    "print(CategoryDataCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then made a barchart from these lists. The bar chart showed me the 6 most clicked on built in functions. These are; My Music, Entertainment, My Interests, Single Player Games, Play Games and Videos.\n",
    "\n",
    "Entertainment is the most clicked on by nearly double the rest. This leads me to believe that Entertainment might have subcategories within it suchas; My Music or Games. However I cannot be sure without asking the data holder.\n",
    "\n",
    "Even though I cannot be sure what 'Entertainment' is exactly, there is a clear corrilation that for the most part people are using the tablet mainly for Music, Games and watching Videos. This is where I will analyse deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = CategoryDataCounter.keys()\n",
    "y = CategoryDataCounter.values()\n",
    "\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90,)\n",
    "plt.bar(x, y)\n",
    "print('Figure 2: Built in Functions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to look into how the users are listening to music. I started by looking at what types of radio stations the users are listening to.\n",
    "I did this by creating a list of all the LinkTitles that include the phrase 'Radio'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadioData = []\n",
    "for i in df.index:\n",
    "    if type(df.at[i, 'LinkTitle']) is str:\n",
    "        if df.at[i, 'LinkTitle'].count('Radio') > 0:\n",
    "            RadioData.append(df.at[i, 'LinkTitle'])\n",
    "\n",
    "print(RadioData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I counted how many items there were in the list to get an idea of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(RadioData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadioDataCounter=collections.Counter(RadioData)\n",
    "\n",
    "print(RadioDataCounter.keys())\n",
    "print(RadioDataCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Radio Stations' is the click before the user gets to the radio station. I can tell becuase the amount of stations clicked on is about the same amount as the amount of clicks on 'Radio Sation'. From this I removed 'Radio Stations' from the visulisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalradioclicks = len(RadioData)\n",
    "while 'Radio Stations' in RadioData:\n",
    "    RadioData.remove('Radio Stations')\n",
    "\n",
    "print(RadioData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadioDataCounter=collections.Counter(RadioData)\n",
    "\n",
    "print(RadioDataCounter.keys())\n",
    "print(RadioDataCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this visulisation we can clearly see that 'Smooth Radio' is the most popular radio station by far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = RadioDataCounter.keys()\n",
    "y = RadioDataCounter.values()\n",
    "\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.bar(x, y)\n",
    "print('Figure 3: Radio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perRadioClick = (totalradioclicks/132535)*100\n",
    "print(perRadioClick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to look at what types of things the users were looking at on the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetData = []\n",
    "for i in df.index:\n",
    "    if df.at[i, 'LinkType'] == 'Internet':\n",
    "        InternetData.append(df.at[i, 'LinkTitle'])\n",
    "\n",
    "print(InternetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(InternetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InternetDataCounter=collections.Counter(InternetData)\n",
    "\n",
    "print(InternetDataCounter.keys())\n",
    "print(InternetDataCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were so many diffrent searches that it is hard to see what is looked at the most. Becuase of this I read through the list. Looking at the list there were some pharses that poped up a lot. I decided to do a more focused visulisation for these phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = InternetDataCounter.keys()\n",
    "y = InternetDataCounter.values()\n",
    "\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.bar(x, y)\n",
    "print('Figure 4: Internet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of the users were using the internet to look at sports. Looking throught the data I can see that the phrase 'football' come up alot. I decided to make a visulisation to see which websites users are viewing related to football."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FootballData = []\n",
    "\n",
    "for i in df.index:\n",
    "    if df.at[i, 'LinkType'] == 'Internet':\n",
    "        if type(df.at[i, 'LinkTitle']) is str:\n",
    "            if df.at[i, 'LinkTitle'].count('Football') > 0:\n",
    "                FootballData.append(df.at[i, 'LinkTitle'])\n",
    "\n",
    "print(FootballData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(FootballData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FootballDataCounter=collections.Counter(FootballData)\n",
    "\n",
    "print(FootballDataCounter.keys())\n",
    "print(FootballDataCounter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart shows that people are looking at football news on the Sky  and BBC websites. It also gives insight into which teams the users support. You can also tell that users are from Scotland or the north by the teams that are being searched for 'Scottish Football', 'Rangers Football Club', 'Falkirk Football', 'Kilmarnock Football Club', 'Dundee United Football Club' and 'Blackpool Football Club'. This shows that although the data does not have the specific geographical location of the users, you could still find out this information in other ways. This is an important thing to be aware of in the future with data protection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = FootballDataCounter.keys()\n",
    "y = FootballDataCounter.values()\n",
    "\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=90)\n",
    "plt.bar(x, y)\n",
    "print('Figure 5: Football')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage of clicks related to football."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalfootball = len(FootballData)\n",
    "perFootballClick = (totalfootball/132535)*100\n",
    "print(perFootballClick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring User Data\n",
    "Now I am going to explore the second dataset of user data called ud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to discover a bit more about the user demographics of the tablet. I started by visualising the gender of users. This allowed me to see that there are more female users than male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Gender'].value_counts().plot(kind='bar')\n",
    "print('Figure 6: Gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a bar chart to show me clearly what conditions are the most prodominent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Condition'].value_counts().plot(kind='barh', figsize=(15, 10))\n",
    "print('Figure 7: Condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked out the mean age of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Age'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked out the mode ages of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Age'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked out the median age of users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud['Age'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Connected Databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to calclate how active each user was and make a new row. I did this by counting the number of clicks each user has made and then added this to each users row in ud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFrequency = pd.DataFrame(df.ExternalID.value_counts().reset_index().values, columns=[\"ExternalID\", \"Frequency\"])\n",
    "print(IDFrequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency is telling us that there are only 177 active users. This seems very low considering that we were given data on 696 users. I think this may be because there are lots of null ExternalIDs in the data set. I am now merging the dataset with the ud set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFrequency.ExternalID = IDFrequency.ExternalID.dropna().astype(int)\n",
    "ud.ExternalID = ud.ExternalID.dropna().astype(int)\n",
    "\n",
    "\n",
    "udF = pd.merge(IDFrequency, ud[['ExternalID','Age','Gender','Condition']], left_on='ExternalID', right_on='ExternalID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udF[\"Frequency\"] = pd.to_numeric(udF[\"Frequency\"])\n",
    "\n",
    "print(udF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udF.dropna(subset=['Age'], how='all', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(udF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to see if users age had anything to do with user frequency on the tablet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udFAge = udF[['Age', 'Frequency']]\n",
    "print(udFAge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udF.plot.scatter(x = 'Age', y = 'Frequency')\n",
    "print('Figure 8: Age Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart shows no clear corrilation. You could argue that people aged 50 to 70 are the most frequent users from this chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udF['Condition'].value_counts().plot(kind='bar', figsize=(15, 10))\n",
    "print('Figure 9: Condition Narrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cancer = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Cancer':\n",
    "        Cancer.append(udF.at[i, 'Frequency'])\n",
    "print('Cancer:', Cancer)\n",
    "CancerSum = 0\n",
    "for i in Cancer:\n",
    "    CancerSum = CancerSum + i\n",
    "AverageCancer = CancerSum/len(Cancer)\n",
    "print('Cancer Average:', AverageCancer)\n",
    "\n",
    "\n",
    "Dementia = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Dementia':\n",
    "        Dementia.append(udF.at[i, 'Frequency'])\n",
    "print('Dementia:', Dementia)\n",
    "DementiaSum = 0\n",
    "for i in Dementia:\n",
    "    DementiaSum = DementiaSum + i\n",
    "AverageDementia = DementiaSum/len(Dementia)\n",
    "print('Dementia Average:', AverageDementia)\n",
    "\n",
    "\n",
    "SpinalInjury = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Spinal Injury':\n",
    "        SpinalInjury.append(udF.at[i, 'Frequency'])\n",
    "print('Spinal Injury:', SpinalInjury)\n",
    "SpinalInjurySum = 0\n",
    "for i in SpinalInjury:\n",
    "    SpinalInjurySum = SpinalInjurySum + i\n",
    "AverageSpinalInjury = SpinalInjurySum/len(SpinalInjury)\n",
    "print('Spinal Injury Average:', AverageSpinalInjury)\n",
    "\n",
    "\n",
    "Diabetes = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Diabetes':\n",
    "        Diabetes.append(udF.at[i, 'Frequency'])\n",
    "print('Diabetes:', Diabetes)\n",
    "DiabetesSum = 0\n",
    "for i in Diabetes:\n",
    "    DiabetesSum = DiabetesSum + i\n",
    "AverageDiabetes = DiabetesSum/len(Diabetes)\n",
    "print('Diabetes Average:', AverageDiabetes)\n",
    "\n",
    "\n",
    "Huntington = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == \"Huntington's\":\n",
    "        Huntington.append(udF.at[i, 'Frequency'])\n",
    "print('Huntington:', Huntington)\n",
    "HuntingtonSum = 0\n",
    "for i in Huntington:\n",
    "    HuntingtonSum = HuntingtonSum + i\n",
    "AverageHuntington = HuntingtonSum/len(Huntington)\n",
    "print('Huntingtons Average:', AverageHuntington)\n",
    "\n",
    "\n",
    "Mobility = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Lifelong Mobility Issues':\n",
    "        Mobility.append(udF.at[i, 'Frequency'])\n",
    "print('Lifelong Mobility Issues:', Mobility)\n",
    "MobilitySum = 0\n",
    "for i in Mobility:\n",
    "    MobilitySum = MobilitySum + i\n",
    "AverageMobility = MobilitySum/len(Mobility)\n",
    "print('Lifelong Mobility Issues Average:', AverageMobility)\n",
    "\n",
    "\n",
    "Arthritis = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Arthritis':\n",
    "        Arthritis.append(udF.at[i, 'Frequency'])\n",
    "print('Arthritis:', Arthritis)\n",
    "ArthritisSum = 0\n",
    "for i in Arthritis:\n",
    "    ArthritisSum = ArthritisSum + i\n",
    "AverageArthritis = ArthritisSum/len(Arthritis)\n",
    "print('Arthritis Average:', AverageArthritis)\n",
    "\n",
    "\n",
    "Asthma = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Asthma':\n",
    "        Asthma.append(udF.at[i, 'Frequency'])\n",
    "print('Asthma:', Asthma)\n",
    "AsthmaSum = 0\n",
    "for i in Asthma:\n",
    "    AsthmaSum = AsthmaSum + i\n",
    "AverageAsthma = AsthmaSum/len(Asthma)\n",
    "print('Asthma Average:', AverageAsthma)\n",
    "\n",
    "\n",
    "MuscularDystrophy = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Muscular Dystrophy':\n",
    "        MuscularDystrophy.append(udF.at[i, 'Frequency'])\n",
    "print('Muscular Dystrophy:', MuscularDystrophy)\n",
    "MuscularDystrophySum = 0\n",
    "for i in MuscularDystrophy:\n",
    "    MuscularDystrophySum = MuscularDystrophySum + i\n",
    "AverageMuscularDystrophy = MuscularDystrophySum/len(MuscularDystrophy)\n",
    "print('Muscular Dystrophy Average:', AverageMuscularDystrophy)\n",
    "\n",
    "\n",
    "MultipleSclerosis = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Multiple Sclerosis':\n",
    "        MultipleSclerosis.append(udF.at[i, 'Frequency'])\n",
    "print('Multiple Sclerosis:', MultipleSclerosis)\n",
    "MultipleSclerosisSum = 0\n",
    "for i in MultipleSclerosis:\n",
    "    MultipleSclerosisSum = MultipleSclerosisSum + i\n",
    "AverageMultipleSclerosis = MultipleSclerosisSum/len(MultipleSclerosis)\n",
    "print('Multiple Sclerosis Average:', AverageMultipleSclerosis)\n",
    "\n",
    "\n",
    "Learning = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Learning Difficulties':\n",
    "        Learning.append(udF.at[i, 'Frequency'])\n",
    "print('Learning Difficulties:', Learning)\n",
    "LearningSum = 0\n",
    "for i in Learning:\n",
    "    LearningSum = LearningSum + i\n",
    "AverageLearning = LearningSum/len(Learning)\n",
    "print('Learning Difficulties Average:', AverageLearning)\n",
    "\n",
    "\n",
    "SpinaBifida = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Spina Bifida':\n",
    "        SpinaBifida.append(udF.at[i, 'Frequency'])\n",
    "print('Spina Bifida:', SpinaBifida)\n",
    "SpinaBifidaSum = 0\n",
    "for i in SpinaBifida:\n",
    "    SpinaBifidaSum = SpinaBifidaSum + i\n",
    "AverageSpinaBifida = SpinaBifidaSum/len(SpinaBifida)\n",
    "print('Spina Bifida Average:', AverageSpinaBifida)\n",
    "\n",
    "\n",
    "Stroke = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Stroke':\n",
    "        Stroke.append(udF.at[i, 'Frequency'])\n",
    "print('Stroke:', Stroke)\n",
    "StrokeSum = 0\n",
    "for i in Stroke:\n",
    "    StrokeSum = StrokeSum + i\n",
    "AverageStroke = StrokeSum/len(Stroke)\n",
    "print('Stroke Average:', AverageStroke)\n",
    "\n",
    "\n",
    "BrainInjury = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Brain Injury':\n",
    "        BrainInjury.append(udF.at[i, 'Frequency'])\n",
    "print('Brain Injury:', BrainInjury)\n",
    "BrainInjurySum = 0\n",
    "for i in BrainInjury:\n",
    "    BrainInjurySum = BrainInjurySum + i\n",
    "AverageBrainInjury = BrainInjurySum/len(BrainInjury)\n",
    "print('Brain Injury Average:', AverageBrainInjury)\n",
    "\n",
    "\n",
    "Epilepsy = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Epilepsy':\n",
    "        Epilepsy.append(udF.at[i, 'Frequency'])\n",
    "print('Epilepsy:', Epilepsy)\n",
    "EpilepsySum = 0\n",
    "for i in Epilepsy:\n",
    "    EpilepsySum = EpilepsySum + i\n",
    "AverageEpilepsy = EpilepsySum/len(Epilepsy)\n",
    "print('Epilepsy Average:', AverageEpilepsy)\n",
    "\n",
    "\n",
    "Elderly = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Elderly Care/Support':\n",
    "        Elderly.append(udF.at[i, 'Frequency'])\n",
    "print('Elderly Care/Support:', Elderly)\n",
    "ElderlySum = 0\n",
    "for i in Elderly:\n",
    "    ElderlySum = ElderlySum + i\n",
    "AverageElderly = ElderlySum/len(Elderly)\n",
    "print('Elderly Care/Support Average:', AverageElderly)\n",
    "\n",
    "\n",
    "Cerebral = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Condition'] == 'Cerebral Palsy':\n",
    "        Cerebral.append(udF.at[i, 'Frequency'])\n",
    "print('Cerebral Palsy:', Cerebral)\n",
    "CerebralSum = 0\n",
    "for i in Cerebral:\n",
    "    CerebralSum = CerebralSum + i\n",
    "AverageCerebral = CerebralSum/len(Cerebral)\n",
    "print('Cerebral Palsy Average:', AverageCerebral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditionTop=[('Cancer',AverageCancer),('Dementia',AverageDementia),('SpinalInjury',AverageSpinalInjury),('Diabetes',AverageDiabetes),(\"Huntington's\",AverageHuntington),('Lifelong Mobility Issues',AverageMobility),('Arthritis',AverageArthritis),('Asthma',AverageAsthma),('Muscular Dystrophy',AverageMuscularDystrophy),('Multiple Sclerosis',AverageMultipleSclerosis),('Learning Difficulties',AverageLearning),('Spina Bifida',AverageSpinaBifida),('Stroke',AverageStroke),('Brain Injury',AverageBrainInjury),('Epilepsy',AverageEpilepsy),('Elderly Care/Support',AverageElderly),('Cerebral Palsy',AverageCerebral)]\n",
    "\n",
    "labels, ys = zip(*conditionTop)\n",
    "xs = np.arange(len(labels)) \n",
    "width = 0.8\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(xs, ys, width, align='center')\n",
    "plt.xtriks(-45)\n",
    "#plt.xticks(xs, labels) #Replace default x-ticks with xs, then replace xs with labels\n",
    "plt.yticks(ys)\n",
    "print('Figure 10: Condition Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I worked out the mean frequency of users by gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udFemale = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Gender'] == 'F':\n",
    "        udFemale.append(udF.at[i, 'Frequency'])\n",
    "print(udFemale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FemaleFSum = 0\n",
    "for i in udFemale:\n",
    "    FemaleFSum = FemaleFSum + i\n",
    "AverageFFemale = FemaleFSum/len(udFemale)\n",
    "print(AverageFFemale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udMale = []\n",
    "for i in udF.index:\n",
    "    if udF.at[i, 'Gender'] == 'M':\n",
    "        udMale.append(udF.at[i, 'Frequency'])\n",
    "print(udMale)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MaleFSum = 0\n",
    "for i in udMale:\n",
    "    MaleFSum = MaleFSum + i\n",
    "    \n",
    "AverageFMale = MaleFSum/len(udMale)\n",
    "print(AverageFMale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top=[('Female',AverageFFemale),('Male',AverageFMale)]\n",
    "\n",
    "labels, ys = zip(*top)\n",
    "xs = np.arange(len(labels)) \n",
    "width = 0.8\n",
    "\n",
    "plt.bar(xs, ys, width, align='center')\n",
    "\n",
    "plt.xticks(xs, labels) #Replace default x-ticks with xs, then replace xs with labels\n",
    "plt.yticks(ys)\n",
    "print('Figure 11: Gender Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can see that Female users are slightly more active than Male users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflect and Hypothesise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reflection:\n",
    "The data was not as clean as I initially thought. Every time I thought I was finished cleaning it as soon as I tried to explore it, I would run into issues. I do not think it is 100% perfectly clean now however I believe I cleaned it to a sensible point. If there was more time, I would have liked to have asked the data holder more questions about the data, so I could fully understand the Categories and be 100% sure of my hypothesis. Since there was not a lot of numerical data is was hard to create interesting diagrams like scatter graphs. However, I believe I have created the desired visualisations that the data holder was hoping to receive for their proposal.\n",
    "\n",
    "I am not sure that each dataset was complete. There were many missing values from both sets. and once I was done merging the datasets there was only 153 rows left. This could have been due to many factors; users are not using the tablet, I dropped a lot of rows due to incomplete rows with NaN values etc. This will have definatly squed the results of my analysis.\n",
    "\n",
    "Hypothesis 1: Football\n",
    "Figure 5: Football\n",
    "The users of this tablet are very interested in seeing the Scottland football scottish premiership results. I can see this in figure 5. All the searched for teams were Scottish. 4.3% of all clicks were football related. This is something that could be used to improve the tablet. Using a BBC News API the developers could create a built in dashboard or app that allows users to view the scores of the football. I could test this hypothisis by conducting interviews with the users of the application to see if they use the tablet to look up football scores. Using advanced alogrithums I would probably find more clicks to do with football - diffrently worded searches or playing football related games etc.\n",
    "\n",
    "Hypothesis 2: Radio\n",
    "Figure 3: Radio\n",
    "The users are using the built in Radio function. However they may need more choice. In figure 3 it shows clearly a huge trend towards Smooth Radio. I suspect this trend is there because this is the only radio staion under the radio category. I think if more choice was given then users would be much more satisfied and be able to have variety. Users are interested in the radio section as about 3.7% of clicks were radio related. I would test this hypothesis by asking the data owner and looking at the tablet.\n",
    "\n",
    "Hypothesis 3: Condiditon\n",
    "Figure 9: Condiditon Narrow\n",
    "Figure 10: Condiditon Frequency\n",
    "People who can use other tech are not using the tablet frequently. I believe that frequency of use is very much dependent on the type of disablity the user has. Looking at figure 9 and 10 and compareing them I can see that there are cases where there are sometimes more people with a certain condition using the tablet less than a small group of  people using it very frequently. An example would be people with learning dificulties. There are around 9 people with learning dificulties using the tablet with an average frequency of about 100. However, there are about 5 people with Arthritis with an average freqency of 900. I believe that this tablet is not suited to some of the people using it. The developers need to idetify a specific target audience and tailor it towards these people and thier needs. This way the user expereince will be better for these people who have no other tech alternative. I would prove this hypothisis by conducting a questionaire with the users, asking the data host and runinng more advanced algorithms on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
